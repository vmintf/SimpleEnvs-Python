#!/usr/bin/env python3
"""
SimpleEnvs vs python-dotenv ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
"""

import os
import time
import tempfile
import statistics
from pathlib import Path
from typing import List, Dict, Any

# ë²¤ì¹˜ë§ˆí¬ ëŒ€ìƒë“¤
try:
    from dotenv import load_dotenv as dotenv_load

    DOTENV_AVAILABLE = True
except ImportError:
    print("âš ï¸  python-dotenvê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install python-dotenv")
    DOTENV_AVAILABLE = False

try:
    from simpleenvs import load_dotenv as simpleenvs_load

    SIMPLEENVS_AVAILABLE = True
except ImportError:
    print("âš ï¸  simpleenvs-pythonì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    SIMPLEENVS_AVAILABLE = False


class BenchmarkRunner:
    """ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ê¸°"""

    def __init__(self, rounds: int = 10):
        self.rounds = rounds
        self.results = {}

    def create_test_env_file(self, var_count: int, file_suffix: str = "") -> str:
        """í…ŒìŠ¤íŠ¸ìš© .env íŒŒì¼ ìƒì„±"""
        # ì„ì‹œ íŒŒì¼ ìƒì„±
        fd, path = tempfile.mkstemp(suffix=f".env{file_suffix}", text=True)

        try:
            with os.fdopen(fd, 'w') as f:
                f.write("# Test .env file\n")
                f.write("# Generated for benchmarking\n\n")

                for i in range(var_count):
                    if i % 4 == 0:
                        f.write(f"VAR_{i:04d}=string_value_{i}\n")
                    elif i % 4 == 1:
                        f.write(f"NUM_{i:04d}={i}\n")
                    elif i % 4 == 2:
                        f.write(f"BOOL_{i:04d}={'true' if i % 2 == 0 else 'false'}\n")
                    else:
                        f.write(f"PATH_{i:04d}=/path/to/file_{i}.txt\n")

                # ì¼ë¶€ ë³µì¡í•œ ê°’ë“¤ ì¶”ê°€
                f.write('DB_URL="postgresql://user:pass@localhost:5432/mydb"\n')
                f.write("API_KEY=abc123def456ghi789\n")
                f.write("DEBUG=true\n")
                f.write("PORT=8080\n")
                f.write("TIMEOUT=30.5\n")

        except Exception:
            os.unlink(path)
            raise

        return path

    def clear_env_vars(self, prefix_list: List[str]):
        """í…ŒìŠ¤íŠ¸ í™˜ê²½ë³€ìˆ˜ë“¤ ì •ë¦¬"""
        keys_to_remove = []
        for key in os.environ:
            for prefix in prefix_list:
                if key.startswith(prefix):
                    keys_to_remove.append(key)

        for key in keys_to_remove:
            del os.environ[key]

    def measure_function(self, func, *args, **kwargs) -> float:
        """í•¨ìˆ˜ ì‹¤í–‰ ì‹œê°„ ì¸¡ì •"""
        start_time = time.perf_counter()
        func(*args, **kwargs)
        end_time = time.perf_counter()
        return end_time - start_time

    def run_benchmark(self, name: str, func, test_file: str, rounds: int = None) -> Dict[str, float]:
        """ê°œë³„ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        if rounds is None:
            rounds = self.rounds

        times = []
        prefixes = ['VAR_', 'NUM_', 'BOOL_', 'PATH_', 'DB_', 'API_', 'DEBUG', 'PORT', 'TIMEOUT']

        print(f"ğŸ”„ {name} ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì¤‘... (rounds: {rounds})")

        for i in range(rounds):
            # í™˜ê²½ë³€ìˆ˜ ì •ë¦¬
            self.clear_env_vars(prefixes)

            # ì‹¤í–‰ ì‹œê°„ ì¸¡ì •
            exec_time = self.measure_function(func, test_file)
            times.append(exec_time)

            if (i + 1) % max(1, rounds // 4) == 0:
                print(f"  Progress: {i + 1}/{rounds}")

        # í†µê³„ ê³„ì‚°
        return {
            'mean': statistics.mean(times),
            'median': statistics.median(times),
            'min': min(times),
            'max': max(times),
            'stdev': statistics.stdev(times) if len(times) > 1 else 0,
            'times': times
        }

    def compare_performance(self, var_count: int) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë¹„êµ ì‹¤í–‰"""
        print(f"\nğŸ“Š {var_count}ê°œ ë³€ìˆ˜ íŒŒì¼ë¡œ ë²¤ì¹˜ë§ˆí¬...")

        # í…ŒìŠ¤íŠ¸ íŒŒì¼ ìƒì„±
        test_file = self.create_test_env_file(var_count)

        try:
            results = {
                'var_count': var_count,
                'file_size': Path(test_file).stat().st_size
            }

            # python-dotenv ë²¤ì¹˜ë§ˆí¬
            if DOTENV_AVAILABLE:
                results['dotenv'] = self.run_benchmark(
                    "python-dotenv",
                    dotenv_load,
                    test_file
                )

            # simpleenvs ë²¤ì¹˜ë§ˆí¬
            if SIMPLEENVS_AVAILABLE:
                results['simpleenvs'] = self.run_benchmark(
                    "SimpleEnvs",
                    simpleenvs_load,
                    test_file
                )

            return results

        finally:
            # í…ŒìŠ¤íŠ¸ íŒŒì¼ ì •ë¦¬
            try:
                os.unlink(test_file)
            except OSError:
                pass

    def print_results(self, results: Dict[str, Any]):
        """ê²°ê³¼ ì¶œë ¥"""
        var_count = results['var_count']
        file_size = results['file_size']

        print(f"\n{'=' * 60}")
        print(f"ğŸ“ˆ ê²°ê³¼: {var_count}ê°œ ë³€ìˆ˜ (íŒŒì¼ í¬ê¸°: {file_size:,} bytes)")
        print(f"{'=' * 60}")

        if 'dotenv' in results and 'simpleenvs' in results:
            dotenv_mean = results['dotenv']['mean']
            simpleenvs_mean = results['simpleenvs']['mean']

            print(f"ğŸ python-dotenv:")
            print(f"   í‰ê· : {dotenv_mean * 1000:.3f}ms")
            print(f"   ì¤‘ê°„ê°’: {results['dotenv']['median'] * 1000:.3f}ms")
            print(f"   ìµœì†Œ/ìµœëŒ€: {results['dotenv']['min'] * 1000:.3f}ms / {results['dotenv']['max'] * 1000:.3f}ms")

            print(f"\nâš¡ SimpleEnvs:")
            print(f"   í‰ê· : {simpleenvs_mean * 1000:.3f}ms")
            print(f"   ì¤‘ê°„ê°’: {results['simpleenvs']['median'] * 1000:.3f}ms")
            print(
                f"   ìµœì†Œ/ìµœëŒ€: {results['simpleenvs']['min'] * 1000:.3f}ms / {results['simpleenvs']['max'] * 1000:.3f}ms")

            # ë¹„êµ
            ratio = dotenv_mean / simpleenvs_mean
            if ratio > 1:
                print(f"\nğŸ† SimpleEnvsê°€ {ratio:.2f}ë°° ë¹ ë¦„!")
            else:
                print(f"\nğŸ† python-dotenvê°€ {1 / ratio:.2f}ë°° ë¹ ë¦„!")

        elif 'dotenv' in results:
            print(f"ğŸ python-dotenv: {results['dotenv']['mean'] * 1000:.3f}ms")

        elif 'simpleenvs' in results:
            print(f"âš¡ SimpleEnvs: {results['simpleenvs']['mean'] * 1000:.3f}ms")

    def run_comprehensive_benchmark(self):
        """ì¢…í•© ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        print("ğŸš€ SimpleEnvs vs python-dotenv ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬")
        print("=" * 60)

        if not (DOTENV_AVAILABLE or SIMPLEENVS_AVAILABLE):
            print("âŒ ë²¤ì¹˜ë§ˆí¬í•  ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        # ë‹¤ì–‘í•œ í¬ê¸°ë¡œ í…ŒìŠ¤íŠ¸
        test_sizes = [10, 50, 100, 500, 1000, 5000]
        all_results = []

        for size in test_sizes:
            try:
                result = self.compare_performance(size)
                all_results.append(result)
                self.print_results(result)

            except Exception as e:
                print(f"âŒ {size}ê°œ ë³€ìˆ˜ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

        # ìš”ì•½ ì¶œë ¥
        self.print_summary(all_results)

    def print_summary(self, all_results: List[Dict[str, Any]]):
        """ì „ì²´ ê²°ê³¼ ìš”ì•½"""
        if not all_results:
            return

        print(f"\n{'=' * 60}")
        print("ğŸ“Š ì „ì²´ ìš”ì•½")
        print(f"{'=' * 60}")

        print(f"{'ë³€ìˆ˜ ê°œìˆ˜':>8} | {'íŒŒì¼í¬ê¸°':>10} | {'dotenv(ms)':>12} | {'SimpleEnvs(ms)':>15} | {'ë¹„ìœ¨':>8}")
        print("-" * 70)

        for result in all_results:
            var_count = result['var_count']
            file_size = result['file_size']

            dotenv_time = result.get('dotenv', {}).get('mean', 0) * 1000
            simpleenvs_time = result.get('simpleenvs', {}).get('mean', 0) * 1000

            if dotenv_time > 0 and simpleenvs_time > 0:
                ratio = dotenv_time / simpleenvs_time
                print(
                    f"{var_count:>8} | {file_size:>8}B | {dotenv_time:>10.3f} | {simpleenvs_time:>13.3f} | {ratio:>6.2f}x")
            elif dotenv_time > 0:
                print(f"{var_count:>8} | {file_size:>8}B | {dotenv_time:>10.3f} | {'N/A':>13} | {'N/A':>8}")
            elif simpleenvs_time > 0:
                print(f"{var_count:>8} | {file_size:>8}B | {'N/A':>10} | {simpleenvs_time:>13.3f} | {'N/A':>8}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse

    parser = argparse.ArgumentParser(description="SimpleEnvs vs python-dotenv ë²¤ì¹˜ë§ˆí¬")
    parser.add_argument("--rounds", "-r", type=int, default=10, help="ê° í…ŒìŠ¤íŠ¸ ë¼ìš´ë“œ ìˆ˜")
    parser.add_argument("--size", "-s", type=int, help="íŠ¹ì • í¬ê¸°ë¡œë§Œ í…ŒìŠ¤íŠ¸ (ë³€ìˆ˜ ê°œìˆ˜)")
    parser.add_argument("--quick", "-q", action="store_true", help="ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (3ë¼ìš´ë“œ)")

    args = parser.parse_args()

    if args.quick:
        args.rounds = 3

    runner = BenchmarkRunner(rounds=args.rounds)

    if args.size:
        # íŠ¹ì • í¬ê¸°ë§Œ í…ŒìŠ¤íŠ¸
        result = runner.compare_performance(args.size)
        runner.print_results(result)
    else:
        # ì¢…í•© ë²¤ì¹˜ë§ˆí¬
        runner.run_comprehensive_benchmark()


if __name__ == "__main__":
    main()