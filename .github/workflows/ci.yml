# .github/workflows/ci.yml
name: CI Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    name: Test Python ${{ matrix.python-version }} on ${{ matrix.os }}
    runs-on: ${{ matrix.os }}
    timeout-minutes: 10
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        python-version: ["3.8", "3.9", "3.10", "3.11", "3.12"]
        exclude:
          # Reduce CI time by skipping some combinations
          - os: windows-latest
            python-version: "3.8"
          - os: macos-latest
            python-version: "3.8"

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}

    - name: Install core dependencies
      run: |
        python -m pip install --upgrade pip
        pip install aiofiles

    - name: Setup Python path
      shell: bash
      run: |
        echo "PYTHONPATH=$PYTHONPATH:$(pwd)/src" >> $GITHUB_ENV
        # Fix Windows encoding issues
        echo "PYTHONIOENCODING=utf-8" >> $GITHUB_ENV
        echo "PYTHONUTF8=1" >> $GITHUB_ENV

    - name: Test import and basic functionality
      shell: bash
      run: |
        python -c "
        import sys
        sys.path.insert(0, './src')
        
        print('Python version:', sys.version)
        print('Platform:', sys.platform)
        
        try:
            import simpleenvs
            print('simpleenvs import successful!')
            
            # Check version
            if hasattr(simpleenvs, '__version__'):
                print('Version:', simpleenvs.__version__)
            elif hasattr(simpleenvs, 'constants'):
                print('Version:', simpleenvs.constants.VERSION)
            else:
                print('Version info not found, but import works!')
            
            # Test basic functionality
            import tempfile, os
            
            # Test load_dotenv if available
            if hasattr(simpleenvs, 'load_dotenv'):
                print('Testing load_dotenv...')
                
                with tempfile.NamedTemporaryFile(mode='w', suffix='.env', delete=False) as f:
                    f.write('TEST_VAR=hello\\nDEBUG=true\\nPORT=8080\\n')
                    env_file = f.name
                
                try:
                    simpleenvs.load_dotenv(env_file)
                    
                    if os.getenv('TEST_VAR') == 'hello':
                        print('Environment loading works!')
                    else:
                        print('Environment variables not loaded properly')
                        
                except Exception as e:
                    print('Function test failed:', str(e))
                finally:
                    os.unlink(env_file)
            else:
                print('load_dotenv not found')
                available_funcs = [x for x in dir(simpleenvs) if not x.startswith('_')]
                print('Available functions:', available_funcs[:10])
            
            print('All tests passed for Python ${{ matrix.python-version }} on ${{ matrix.os }}!')
            
        except ImportError as e:
            print('Import failed:', str(e))
            import os
            if os.path.exists('./src'):
                print('src/ contents:', os.listdir('./src'))
            sys.exit(1)
        except Exception as e:
            print('Unexpected error:', str(e))
            sys.exit(1)
        "

    - name: Test async functionality (if supported)
      shell: bash
      run: |
        python -c "
        import sys
        sys.path.insert(0, './src')
        import simpleenvs
        import asyncio
        import tempfile
        import os
        
        async def test_async():
            try:
                if hasattr(simpleenvs, 'load') or hasattr(simpleenvs, 'aload_dotenv'):
                    print('Testing async functionality...')
                    
                    with tempfile.NamedTemporaryFile(mode='w', suffix='.env', delete=False) as f:
                        f.write('ASYNC_TEST=async_works\\n')
                        env_file = f.name
                    
                    try:
                        if hasattr(simpleenvs, 'load'):
                            await simpleenvs.load(env_file)
                        elif hasattr(simpleenvs, 'aload_dotenv'):
                            await simpleenvs.aload_dotenv(env_file)
                        
                        print('Async loading works!')
                    except Exception as e:
                        print('Async test failed:', str(e))
                    finally:
                        os.unlink(env_file)
                else:
                    print('No async functions found')
            except Exception as e:
                print('Async test error:', str(e))
        
        try:
            asyncio.run(test_async())
        except Exception as e:
            print('Could not run async test:', str(e))
        "

  build:
    name: Build Package
    runs-on: ubuntu-latest
    needs: test
    timeout-minutes: 10

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: "3.11"

    - name: Install build dependencies
      run: |
        python -m pip install --upgrade pip
        pip install build twine aiofiles

    - name: Verify package can be imported before build
      run: |
        export PYTHONPATH="$PYTHONPATH:$(pwd)/src"
        python -c "
        import sys
        sys.path.insert(0, './src')
        import simpleenvs
        print('‚úÖ Package imports successfully before build')
        "

    - name: Build package
      run: |
        python -m build
        echo "‚úÖ Package built successfully!"

    - name: Check package
      run: |
        twine check dist/*
        echo "‚úÖ Package check passed!"

    - name: Test wheel installation
      run: |
        # Install the wheel
        pip install dist/*.whl
        
        # Test import after installation
        python -c "
        try:
            import simpleenvs
            print('‚úÖ Wheel installation and import successful!')
        except ImportError as e:
            print(f'‚ùå Wheel import failed: {e}')
            exit(1)
        "

    - name: Upload build artifacts
      uses: actions/upload-artifact@v3
      with:
        name: dist-${{ github.sha }}
        path: dist/
        retention-days: 30

---

# .github/workflows/deploy.yml
name: Deploy to PyPI

on:
  release:
    types: [published]
  workflow_dispatch:
    inputs:
      environment:
        description: 'Deployment target'
        required: true
        default: 'testpypi'
        type: choice
        options:
          - testpypi
          - pypi

jobs:
  deploy:
    name: Deploy to ${{ github.event.inputs.environment || 'PyPI' }}
    runs-on: ubuntu-latest
    timeout-minutes: 15
    environment: ${{ github.event.inputs.environment || 'pypi' }}

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: "3.11"

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install build twine aiofiles

    - name: Pre-deployment test
      run: |
        export PYTHONPATH="$PYTHONPATH:$(pwd)/src"
        python -c "
        import sys
        sys.path.insert(0, './src')
        import simpleenvs
        
        # Get version
        if hasattr(simpleenvs, '__version__'):
            version = simpleenvs.__version__
        elif hasattr(simpleenvs, 'constants'):
            version = simpleenvs.constants.VERSION
        else:
            version = 'unknown'
            
        print(f'üì¶ Deploying SimpleEnvs version: {version}')
        
        # Basic functionality test
        import tempfile, os
        if hasattr(simpleenvs, 'load_dotenv'):
            with tempfile.NamedTemporaryFile(mode='w', suffix='.env', delete=False) as f:
                f.write('DEPLOY_TEST=works\\n')
                env_file = f.name
            
            try:
                simpleenvs.load_dotenv(env_file)
                if os.getenv('DEPLOY_TEST') == 'works':
                    print('‚úÖ Pre-deployment test passed!')
                else:
                    print('‚ùå Pre-deployment test failed!')
                    exit(1)
            finally:
                os.unlink(env_file)
        "

    - name: Build package
      run: |
        python -m build
        echo "‚úÖ Package built for deployment!"

    - name: Check package
      run: |
        twine check dist/*
        echo "‚úÖ Package validation passed!"

    - name: Deploy to Test PyPI
      if: github.event.inputs.environment == 'testpypi'
      env:
        TWINE_USERNAME: __token__
        TWINE_PASSWORD: ${{ secrets.TEST_PYPI_API_TOKEN }}
      run: |
        twine upload --repository testpypi dist/*
        echo "üß™ Successfully deployed to Test PyPI!"

    - name: Deploy to PyPI
      if: github.event.inputs.environment == 'pypi' || github.event_name == 'release'
      env:
        TWINE_USERNAME: __token__
        TWINE_PASSWORD: ${{ secrets.PYPI_API_TOKEN }}
      run: |
        twine upload dist/*
        echo "üöÄ Successfully deployed to PyPI!"

    - name: Create deployment summary
      run: |
        echo "## üöÄ Deployment Summary" >> $GITHUB_STEP_SUMMARY
        echo "- **Target**: ${{ github.event.inputs.environment || 'PyPI' }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Commit**: ${{ github.sha }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Trigger**: ${{ github.event_name }}" >> $GITHUB_STEP_SUMMARY

---

# .github/workflows/benchmark.yml
name: Performance Benchmark

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:
  schedule:
    - cron: '0 2 * * 1'  # Weekly on Monday

jobs:
  benchmark:
    name: Performance Benchmark
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: "3.11"

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install aiofiles python-dotenv

    - name: Setup Python path
      run: |
        echo "PYTHONPATH=$PYTHONPATH:$(pwd)/src" >> $GITHUB_ENV

    - name: Verify both packages work
      run: |
        python -c "
        import sys
        sys.path.insert(0, './src')
        
        # Test SimpleEnvs
        import simpleenvs
        print('‚úÖ SimpleEnvs imported successfully')
        
        # Test python-dotenv
        import dotenv
        print('‚úÖ python-dotenv imported successfully')
        print(f'SimpleEnvs: {getattr(simpleenvs, \"__version__\", \"unknown\")}')
        print(f'python-dotenv: {getattr(dotenv, \"__version__\", \"unknown\")}')
        "

    - name: Run performance benchmark
      run: |
        python -c "
        import time
        import tempfile
        import os
        import sys
        sys.path.insert(0, './src')
        
        def create_test_env(num_vars=100):
            content = []
            for i in range(num_vars):
                content.append(f'VAR_{i}=value_{i}_test_data')
            
            with tempfile.NamedTemporaryFile(mode='w', suffix='.env', delete=False) as f:
                f.write('\n'.join(content))
                return f.name

        def benchmark_simpleenvs(env_file, iterations=5):
            import simpleenvs
            times = []
            
            for _ in range(iterations):
                # Clear environment
                for key in list(os.environ.keys()):
                    if key.startswith('VAR_'):
                        del os.environ[key]
                
                start = time.perf_counter()
                simpleenvs.load_dotenv(env_file)
                end = time.perf_counter()
                times.append((end - start) * 1000)
            
            return times

        def benchmark_python_dotenv(env_file, iterations=5):
            from dotenv import load_dotenv
            times = []
            
            for _ in range(iterations):
                # Clear environment
                for key in list(os.environ.keys()):
                    if key.startswith('VAR_'):
                        del os.environ[key]
                
                start = time.perf_counter()
                load_dotenv(env_file, override=True)
                end = time.perf_counter()
                times.append((end - start) * 1000)
            
            return times

        print('üöÄ SimpleEnvs vs python-dotenv Performance Benchmark')
        print('=' * 60)
        
        test_sizes = [50, 100, 500]
        
        for size in test_sizes:
            print(f'\nüìä Testing with {size} variables...')
            
            env_file = create_test_env(size)
            
            try:
                # Benchmark both
                simpleenvs_times = benchmark_simpleenvs(env_file, iterations=3)
                dotenv_times = benchmark_python_dotenv(env_file, iterations=3)
                
                # Calculate averages
                avg_simpleenvs = sum(simpleenvs_times) / len(simpleenvs_times)
                avg_dotenv = sum(dotenv_times) / len(dotenv_times)
                speedup = avg_dotenv / avg_simpleenvs if avg_simpleenvs > 0 else 1.0
                
                print(f'  SimpleEnvs:    {avg_simpleenvs:.2f}ms')
                print(f'  python-dotenv: {avg_dotenv:.2f}ms')
                print(f'  Speedup:       {speedup:.1f}x faster ‚ö°')
                
            except Exception as e:
                print(f'  ‚ùå Benchmark failed: {e}')
            finally:
                os.unlink(env_file)
        
        print('\n‚úÖ Benchmark completed!')
        "

    - name: Upload benchmark results
      uses: actions/upload-artifact@v3
      with:
        name: benchmark-results-${{ github.sha }}
        path: benchmark-results.txt
        retention-days: 30