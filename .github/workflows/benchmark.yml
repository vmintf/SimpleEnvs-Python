# .github/workflows/benchmark.yml
name: Performance Benchmark

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:
  schedule:
    - cron: '0 2 * * 1'  # Weekly on Monday

jobs:
  benchmark:
    name: Performance Benchmark
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: "3.11"

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install aiofiles python-dotenv

    - name: Setup Python path
      run: |
        echo "PYTHONPATH=$PYTHONPATH:$(pwd)/src" >> $GITHUB_ENV

    - name: Verify both packages work
      run: |
        python -c "
        import sys
        sys.path.insert(0, './src')
        
        # Test SimpleEnvs
        import simpleenvs
        print('‚úÖ SimpleEnvs imported successfully')
        
        # Test python-dotenv
        import dotenv
        print('‚úÖ python-dotenv imported successfully')
        print(f'SimpleEnvs: {getattr(simpleenvs, \"__version__\", \"unknown\")}')
        print(f'python-dotenv: {getattr(dotenv, \"__version__\", \"unknown\")}')
        "

    - name: Run performance benchmark
      run: |
        python -c "
        import time
        import tempfile
        import os
        import sys
        sys.path.insert(0, './src')
        
        def create_test_env(num_vars=100):
            content = []
            for i in range(num_vars):
                content.append(f'VAR_{i}=value_{i}_test_data')
            
            with tempfile.NamedTemporaryFile(mode='w', suffix='.env', delete=False) as f:
                f.write('\n'.join(content))
                return f.name

        def benchmark_simpleenvs(env_file, iterations=5):
            import simpleenvs
            times = []
            
            for _ in range(iterations):
                # Clear environment
                for key in list(os.environ.keys()):
                    if key.startswith('VAR_'):
                        del os.environ[key]
                
                start = time.perf_counter()
                simpleenvs.load_dotenv(env_file)
                end = time.perf_counter()
                times.append((end - start) * 1000)
            
            return times

        def benchmark_python_dotenv(env_file, iterations=5):
            from dotenv import load_dotenv
            times = []
            
            for _ in range(iterations):
                # Clear environment
                for key in list(os.environ.keys()):
                    if key.startswith('VAR_'):
                        del os.environ[key]
                
                start = time.perf_counter()
                load_dotenv(env_file, override=True)
                end = time.perf_counter()
                times.append((end - start) * 1000)
            
            return times

        print('üöÄ SimpleEnvs vs python-dotenv Performance Benchmark')
        print('=' * 60)
        
        test_sizes = [50, 100, 500]
        
        for size in test_sizes:
            print(f'\nüìä Testing with {size} variables...')
            
            env_file = create_test_env(size)
            
            try:
                # Benchmark both
                simpleenvs_times = benchmark_simpleenvs(env_file, iterations=3)
                dotenv_times = benchmark_python_dotenv(env_file, iterations=3)
                
                # Calculate averages
                avg_simpleenvs = sum(simpleenvs_times) / len(simpleenvs_times)
                avg_dotenv = sum(dotenv_times) / len(dotenv_times)
                speedup = avg_dotenv / avg_simpleenvs if avg_simpleenvs > 0 else 1.0
                
                print(f'  SimpleEnvs:    {avg_simpleenvs:.2f}ms')
                print(f'  python-dotenv: {avg_dotenv:.2f}ms')
                print(f'  Speedup:       {speedup:.1f}x faster ‚ö°')
                
            except Exception as e:
                print(f'  ‚ùå Benchmark failed: {e}')
            finally:
                os.unlink(env_file)
        
        print('\n‚úÖ Benchmark completed!')
        "

    - name: Upload benchmark results
      uses: actions/upload-artifact@v3
      with:
        name: benchmark-results-${{ github.sha }}
        path: benchmark-results.txt
        retention-days: 30